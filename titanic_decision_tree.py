# -*- coding: utf-8 -*-
"""Titanic decision tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vNUg4vGgszjm0_XdfKmRQ9SsZPmM9L97
"""

#import libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier ##Install the package "scikit-learn"
from sklearn.tree import plot_tree
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn import tree
from matplotlib import pyplot as plt
import os

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("Titanic.csv") #creating dataframe from csv file

df.head(6)

#split dataset in features and target variable
feature_cols = ['Pclass', 'Male', 'Age', 'SibSp', 'Parch', 'Fare'] #list of feature columns (columns to feature/use as deciding variables)
target_col = ['Survived'] #list of target column
X = df[feature_cols] #assign X to new dataframe with only feature_cols filtered columns
y = df[target_col].values.ravel() #Assign Y to 1D array of target column list

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # Split dataset into training and test set (x_train,y_train) and ensures reproducability via random_state=1. (training feature, test feature, training target, test target)

# Decision tree classifier using entropy as split criterion,
# maximum node depth of 10 levels,
# and minimum of 3 samples required to split a node
clf = DecisionTreeClassifier(criterion="entropy", max_depth=10, min_samples_split=3)

results = clf.fit(X_train,y_train) #train/fit classfier based on X_train and y_train. Refer back to clf from above, modifies clf in place and returns trained model

plt.figure(figsize=(40,20)) #sets dimensions to 40x20
tree.plot_tree(results, feature_names = X.columns) #plots decision tree using results, label each node with feature_cols (X)
plt.savefig('treeplot.png') #saves as treeplot.png

y_pred1 = clf.predict(X_train) #use trained model; clf to predict target values for training dataset (X_train)
y_pred2 = clf.predict(X_test) #use same trained model; clf, to predict target values for test dataset (X_test)

print("Accuracy of train dataset(criterion=entropy, max_depth=10, min_samples_split=3):",metrics.accuracy_score(y_train, y_pred1))
print("Accuracy of test dataset(criterion=entropy, max_depth=10, min_samples_split=3):",metrics.accuracy_score(y_test, y_pred2))

# Decision tree classifier using entropy as split criterion,
# maximum node depth of 100 levels,
# and minimum of 3 samples required to split a node
clf = DecisionTreeClassifier(criterion="entropy", max_depth=100, min_samples_split=3)

results = clf.fit(X_train,y_train) #train/fit classfier based on X_train and y_train. Refer back to clf from above, modifies clf in place and returns trained model
plt.figure(figsize=(40,20)) #sets dimensions to 40x20
tree.plot_tree(results, feature_names = X.columns) #plots decision tree using results, label each node with feature_cols (X)
plt.savefig('treeplot2.png') #saves as treeplot2.png
#Predict the response for test dataset
y_pred3 = clf.predict(X_train) #use trained model; clf to predict target values for training dataset (X_train)
y_pred4 = clf.predict(X_test) #use same trained model; clf, to predict target values for test dataset (X_test)

print("Accuracy of train dataset (criterion=entropy, max_depth=100, min_samples_split=3):",metrics.accuracy_score(y_train, y_pred3))
print("Accuracy of test dataset (criterion=entropy, max_depth=100, min_samples_split=3):",metrics.accuracy_score(y_test, y_pred4))

rf_classifier = RandomForestClassifier(n_estimators=10, random_state=1) #create a blank random forest classifer model with 10 decision trees and allow reproducability via random state = 1

rf_classifier.fit(X_train, y_train) #Fit random forest model using training features and training target (X_Train, y_train)

predictions = rf_classifier.predict(X_test) #Predict target labels for the test set using the trained random forest model trained from X_train and y_train

accuracy = accuracy_score(y_test, predictions) #compare y_test data to predicted data (predictions) based on X_test
print(f"Random Forest Accuracy: {accuracy:.2f}")  # Proportion of correct predictions comparing y_test and predicted labels from X_test